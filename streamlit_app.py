
import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import openai
from urllib.parse import urljoin, urlparse
import time
import re
import io


# ---- Helper functions ----
def get_visible_text(html):
    soup = BeautifulSoup(html, "html.parser")
    # Remove scripts/styles
    for script in soup(["script", "style", "noscript"]):
        script.decompose()
    # Get text, filter empty/whitespace
    texts = soup.stripped_strings
    return " ".join([t for t in texts if t and len(t) > 20])

def is_internal_link(base_url, link):
    base_domain = urlparse(base_url).netloc
    target = urljoin(base_url, link)
    return urlparse(target).netloc == base_domain

def is_irrelevant_link(link_text):
    blacklist = [
        "kontakt", "privacy", "polityka", "regulamin", "terms",
        "cookies", "kariera", "praca", "career", "login", "logowanie"
    ]
    return any(b in link_text.lower() for b in blacklist)

def find_relevant_links(base_url, soup, max_links=3):
    links = []
    for a in soup.find_all("a", href=True):
        text = a.get_text(separator=" ", strip=True)
        href = a["href"]
        if is_internal_link(base_url, href) and not is_irrelevant_link(href + " " + text):
            url = urljoin(base_url, href)
            if url not in links and url != base_url:
                links.append(url)
        if len(links) >= max_links:
            break
    return links

def scrape_website(url, max_pages=4, timeout=8):
    try:
        res = requests.get(url, timeout=timeout, headers={'User-Agent': 'Mozilla/5.0'})
        res.raise_for_status()
        html = res.text
        main_text = get_visible_text(html)
        soup = BeautifulSoup(html, "html.parser")
        subpage_links = find_relevant_links(url, soup, max_links=max_pages-1)
        texts = [main_text]
        for link in subpage_links:
            try:
                sub_res = requests.get(link, timeout=timeout, headers={'User-Agent': 'Mozilla/5.0'})
                sub_res.raise_for_status()
                sub_html = sub_res.text
                sub_text = get_visible_text(sub_html)
                texts.append(sub_text)
                time.sleep(0.5)  # be nice to servers!
            except Exception:
                continue
        return "\n".join(texts)
    except Exception:
        return None

def build_investor_comment(company, gpt_reason, gpt_industry):
    # szablon do personalizacji
    template = (
        f"Od pewnego czasu z zainteresowaniem obserwujemy rozwÃ³j spÃ³Å‚ek w branÅ¼y {gpt_industry}. "
        f"{company} znalazÅ‚a siÄ™ w krÄ™gu naszego zainteresowania ze wzglÄ™du na {gpt_reason}, "
        f"ktÃ³re wpisuje siÄ™ w strategiÄ™ inwestycyjnÄ… CMT Family Office. "
        f"RozwaÅ¼amy moÅ¼liwoÅ›Ä‡ zaangaÅ¼owania siÄ™ poprzez dofinansowanie spÃ³Å‚ki lub nabycie pakietu mniejszoÅ›ciowego nawiÄ…zujÄ…c partnerskÄ… wspÃ³Å‚pracÄ™."
    )
    return template
    
def build_prompt(company, text):
    prompt = (
        f"Jako potencjalny inwestor zainteresowany wspÃ³Å‚pracÄ… z firmÄ… \"{company}\", "
        f"napisz w jÄ™zyku polskim krÃ³tkÄ…, profesjonalnÄ… wypowiedÅº (2-3 zdania) skierowanÄ… do zarzÄ…du tej spÃ³Å‚ki. "
        f"PodkreÅ›l, Å¼e zwracamy uwagÄ™ na PaÅ„stwa kompetencje, innowacyjnoÅ›Ä‡ oraz unikalne projekty i rozwiÄ…zania, "
        f"ktÃ³re dostarczacie swoim klientom biznesowym. JeÅ›li spÃ³Å‚ka tworzy technologie lub systemy dla innych firm, "
        f"zaznacz wprost, Å¼e sÄ… PaÅ„stwo twÃ³rcÄ… rozwiÄ…zaÅ„, ktÃ³re wspierajÄ… rozwÃ³j i cyfryzacjÄ™ swoich partnerÃ³w z branÅ¼y, "
        f"ale nie sugeruj, Å¼e sami oferujecie te usÅ‚ugi koÅ„cowym uÅ¼ytkownikom. "
        f"Unikaj ogÃ³lnikÃ³w, pisz zwiÄ™Åºle i rzeczowo, pokazujÄ…c prawdziwÄ… rolÄ™ spÃ³Å‚ki na rynku B2B lub technologicznym. "
        f"Nie pisz nic poza tÄ… wypowiedziÄ…. "
        f"PoniÅ¼ej znajduje siÄ™ opis oraz treÅ›Ä‡ strony:\n\n{text}"
    )
    return prompt

def get_company_industry(scraped_text, client):
    prompt = (
        f"Na podstawie poniÅ¼szego opisu okreÅ›l jednym sÅ‚owem lub krÃ³tkÄ… frazÄ…, w jakiej branÅ¼y dziaÅ‚a ta spÃ³Å‚ka. "
        f"Nie pisz nic wiÄ™cej.\n\n{scraped_text}"
    )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=10
    )
    return response.choices[0].message.content.strip()

def get_company_reason(scraped_text, client):
    prompt = (
        f"Na podstawie poniÅ¼szego opisu napisz w jÄ™zyku polskim jednym zdaniem, co szczegÃ³lnie wyrÃ³Å¼nia tÄ™ spÃ³Å‚kÄ™ "
        f"i dlaczego mogÅ‚a przykuÄ‡ uwagÄ™ inwestora. Nie pisz nic poza tym zdaniem.\n\n{scraped_text}"
    )
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=100
    )
    return response.choices[0].message.content.strip()

# Potem w gÅ‚Ã³wnej pÄ™tli:
industry = get_company_industry(scraped_text, client)
reason = get_company_reason(scraped_text, client)
comment = build_investor_comment(company, scraped_text, reason, industry)

def build_full_email(person, opening, compliment, ending):
    first_name = person.split()[0].title()  # tylko pierwsze sÅ‚owo z duÅ¼ej litery
    email = (
        f"Szanowny Panie {first_name},\n\n"
        f"{opening.strip()}\n\n"
        f"{compliment.strip()}\n\n"
        f"{ending.strip()}"
    )
    return email

# ---- Streamlit App ----
st.set_page_config(page_title="GenAI Cold Email Generator", layout="wide")
st.title("âœ‰ï¸ GenAI: Personalizowane maile coldmailingowe")

st.sidebar.header("ğŸ“ Opcje e-maila")
opening = st.sidebar.text_area("WstÄ™p (Opening)", value=" ")
ending = st.sidebar.text_area("ZakoÅ„czenie (Ending)", value="")

st.sidebar.info("API key OpenAI jest pobierany z `.streamlit/secrets.toml`", icon="ğŸ”‘")

uploaded_file = st.file_uploader("Wgraj plik Excel z kolumnami: Nazwa firmy, ImiÄ™ i nazwisko, Strona internetowa", type=["xls", "xlsx"])

if uploaded_file:
    df = pd.read_excel(uploaded_file)
    st.dataframe(df.head())
    if st.button("Generuj maile"):
        openai.api_key = st.secrets["openai_api_key"]
        results = []
        for idx, row in df.iterrows():
            company = str(row.get("Nazwa firmy", "")).strip()
            person = str(row.get("ImiÄ™ i nazwisko", "")).strip()
            website = str(row.get("Strona internetowa", "")).strip()
            if not (company and person and website):
                continue
            st.info(f"Przetwarzam: {company} ({website})", icon="ğŸ”„")
            scraped = scrape_website(website)
            if scraped is None or len(scraped) < 100:
                st.warning(f"âŒ Nie udaÅ‚o siÄ™ pobraÄ‡ danych z: {website} (pomijam)", icon="âš ï¸")
                continue
            try:
                compliment = generate_gpt_compliment(company, scraped)
                full_email = build_full_email(person, opening, compliment, ending)
                results.append({
                    **row,
                    "Personalizowany email": full_email
                })
            except Exception as e:
                st.error(f"BÅ‚Ä…d generowania AI: {e}")
                continue
            time.sleep(1.1)  # Be polite to OpenAI and websites
        if results:
            result_df = pd.DataFrame(results)
            st.success(f"Wygenerowano {len(result_df)} spersonalizowanych e-maili!")
            st.dataframe(result_df)
            towrite = io.BytesIO()
            result_df.to_excel(towrite, index=False)
            towrite.seek(0)
            st.download_button(
                label="Pobierz wynikowy plik Excel",
                data=towrite,
                file_name="personalizowane_maile.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        else:
            st.error("Nie udaÅ‚o siÄ™ wygenerowaÄ‡ Å¼adnych e-maili. SprawdÅº dane wejÅ›ciowe.")
